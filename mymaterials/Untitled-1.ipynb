{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inductive vs Deductive Reasoning\n",
    "Inductive reasoning involves deriving general principles or patterns from specific observations. For instance, if a person visits several restaurants and notices that all of them serve pizza, they might generalize that pizza is a popular dish in that area.\n",
    "\n",
    "Deductive reasoning, on the other hand, is the process of reaching a specific conclusion based on general principles or premises. For example, if it is known that all birds have feathers, and a robin is a bird, then one can deduce that a robin has feathers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)\n",
      "RandomForestClassifier(min_samples_leaf=4, n_estimators=200, random_state=42)\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    categorical_features = ['workclass', 'education', 'martial-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "    X = df.drop('income', axis=1)\n",
    "    y = df['income']\n",
    "    preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "    X = preprocessor.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "def decision_tree_model(X_train, y_train):\n",
    "    param_grid = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "    grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def random_forest_model(X_train, y_train):\n",
    "    param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_leaf': [1, 2, 4]}\n",
    "    grid = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, n_iter=10, cv=5, random_state=42)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def xgboost_model(X_train, y_train):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    param_grid = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [50, 100, 200]}\n",
    "    grid = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), param_grid, cv=5)\n",
    "    grid.fit(X_train, y_train_encoded)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# Load and preprocess data\n",
    "df = load_data('adult.csv')\n",
    "X, y = preprocess_data(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models\n",
    "dt_model = decision_tree_model(X_train, y_train)\n",
    "rf_model = random_forest_model(X_train, y_train)\n",
    "xgb_model = xgboost_model(X_train, y_train)\n",
    "\n",
    "print(dt_model)\n",
    "print(rf_model)\n",
    "print(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decesion Tree Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Max Depth (10): This means that the decision tree can have a maximum depth of 10 levels. Imagine a tree-like structure where each level represents a decision based on a feature. A depth of 10 implies that the tree can make up to 10 sequential decisions before reaching a prediction.\n",
    "\n",
    "    Min Samples Split (10): This parameter specifies that a node in the tree must have at least 10 samples to be eligible for further splitting. It helps control the complexity of the tree by preventing splits on nodes with too few samples, which could lead to overfitting.\n",
    "    \n",
    "    Random State (42): This ensures that the random process used to build the tree is reproducible. In other words, if you run the model with the same random state multiple times, you'll get the same results each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Learning Rate (0.2): This is the step size shrinkage used in the gradient boosting process. A learning rate of 0.2 means that each tree's contribution to the ensemble is reduced by 0.2 at each iteration, which helps prevent overfitting.\n",
    "\n",
    "    Max Depth (3): Similar to the decision tree, this sets the maximum depth of each individual tree in the ensemble. With a max depth of 3, each tree can only make up to 3 sequential decisions before reaching a prediction.\n",
    "\n",
    "    Number of Estimators (200): Like in the random forest, this determines the number of boosting rounds or trees to build in the ensemble.\n",
    "    \n",
    "    Eval Metric ('logloss'): XGBoost uses the logarithmic loss ('logloss') as the evaluation metric during training. Logloss measures the performance of the model's predicted probabilities against the actual labels.\n",
    "    Random State (42): Ensures reproducibility of results, similar to the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Min Samples Leaf (4): In a random forest, each decision tree is trained on a subset of the data. This parameter specifies the minimum number of samples required to be at a leaf node of each individual tree. Setting it to 4 means that each leaf node must have at least 4 samples.\n",
    "\n",
    "    Number of Estimators (200): Random forests consist of an ensemble of decision trees. This parameter defines how many trees are in the forest. Having 200 trees means that the model aggregates predictions from 200 individual decision trees.\n",
    "    \n",
    "    Random State (42): As with the decision tree, this ensures reproducibility of results across different runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
